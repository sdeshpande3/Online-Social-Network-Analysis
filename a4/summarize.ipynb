{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_file(name):\n",
    "    with open(name + '.pkl', 'rb') as f:\n",
    "        return pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_info():\n",
    "    file = open('summary.txt', 'w')\n",
    "    file.write(\"*****Number of users collected:*****\\n\")\n",
    "    file.write('\\n')\n",
    "    users = load_file('information')\n",
    "    file.write(\"There are %d initial users collected.\\n\" % (len(users)))\n",
    "    file.write(\"All friends of these users are also collected for future analysis.\\n\")\n",
    "    for user in users:\n",
    "        file.write(\"%s has %d friends.\\n\" % (user['screen_name'], len(user['friend_id'])))\n",
    "    file.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tweets():\n",
    "    file = open('summary.txt', 'a')\n",
    "    file.write('*****Number of messages collected:*****\\n')\n",
    "    file.write('\\n')\n",
    "    tweets = load_file('tweets')\n",
    "    file.write('For sentiment analysis, we collected %d tweets (max. tweets allowed)\\n' % len(tweets))\n",
    "    file.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cluster():\n",
    "    file = open('summary.txt', 'a')\n",
    "    file.write('*****Number of communities discovered:*****\\n')\n",
    "    file.write('\\n')\n",
    "    file.write('We cluster all initial users and their friends in to different communities and exclude users that followed by less than two initial users and outliers.\\n')\n",
    "    file.write('Outliers are those points that clustered as singleton.\\n')\n",
    "    clusters = load_file('clusters')\n",
    "    file.write(\"There are %d communities\\n\" % (len(clusters)))\n",
    "#     print(len(pruned))\n",
    "    file.write('\\n')\n",
    "    total = 0\n",
    "    for cluster in clusters:\n",
    "        total += len(cluster.nodes())\n",
    "    avg = total / len(clusters)\n",
    "    file.write('Total number of users in the network: %d\\n' % (total))\n",
    "    file.write('*****Average number of users per community:*****\\n %d\\n' % (avg))\n",
    "    file.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_classify():\n",
    "    file = open('summary.txt', 'a')\n",
    "    file.write('*****Number of instances per class found:*****\\n')\n",
    "    file.write('\\n')\n",
    "    file.write('There are three classes for sentiment analysis.\\n')\n",
    "    (positives,negatives,combine)  = load_file('classify')\n",
    "    file.write('\\nTotal positive, negative and combine tokens: %d positive instances , %d negative instances and %d combine instances'\n",
    "    %(len(positives), len(negatives), len(combine)))\n",
    "    print('\\n')\n",
    "    file.write('\\n\\n*****One example from each class:*****\\n')\n",
    "    for tweet, pos, neg in sorted(positives, key=lambda x: x[1], reverse=False):\n",
    "        positive = (pos,neg,tweet)\n",
    "    \n",
    "    for tweet, pos, neg in sorted(negatives, key=lambda x: x[2], reverse=False):\n",
    "        negative = (neg,pos,tweet)\n",
    "    \n",
    "    for tweet, pos, neg in sorted(combine, key=lambda x: x[2], reverse=True):\n",
    "        combined = (pos,neg,tweet)\n",
    "        \n",
    "    file.write('\\nPOSITIVE:')\n",
    "    content1 = positive[2].encode('utf-8', 'ignore') \n",
    "    file.write(str(content1))\n",
    "    file.write('\\n')\n",
    "    \n",
    "    file.write('\\nNEGATIVE:')\n",
    "    content2 = negative[2].encode('utf-8', 'ignore') \n",
    "    file.write(str(content2))\n",
    "    file.write('\\n')\n",
    "    \n",
    "    file.write('\\nNEUTRAL:')\n",
    "    content3 = combined[2].encode('utf-8', 'ignore') \n",
    "    file.write(str(content3))\n",
    "    \n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Written!\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    create_info()\n",
    "    create_tweets()\n",
    "    create_cluster()\n",
    "    create_classify()\n",
    "    print('Written!')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
